{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ea5f5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-51b4fe851295>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-51b4fe851295>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (setq python-shell-interpreter-args \"--simple-prompt -i\")\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(setq python-shell-interpreter-args \"--simple-prompt -i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-03 00:28:40 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 00:28:40 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.8 (default, Apr 13 2021, 12:59:45) - [Clang 10.0.0 ], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2022-05-03 00:28:40 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
      " 'LOGSTATS_INTERVAL': 0}\n",
      "2022-05-03 00:28:40 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2022-05-03 00:28:40 [scrapy.extensions.telnet] INFO: Telnet Password: 3d84668074f878ef\n",
      "2022-05-03 00:28:40 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage']\n",
      "2022-05-03 00:28:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 00:28:40 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 00:28:40 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 00:28:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 00:28:41 [asyncio] DEBUG: Using selector: KqueueSelector\n",
      "[s] Available Scrapy objects:\n",
      "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
      "[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa7db948580>\n",
      "[s]   item       {}\n",
      "[s]   settings   <scrapy.settings.Settings object at 0x7fa7db948280>\n",
      "[s] Useful shortcuts:\n",
      "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
      "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
      "[s]   shelp()           Shell help (print this help)\n",
      "[s]   view(response)    View response in a browser\n",
      "2022-05-03 00:28:41 [asyncio] DEBUG: Using selector: KqueueSelector\n",
      "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[8D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
      "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! scrapy shell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327dbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!fetch('https://www.cars.com/shopping/results/\\?list_price_max\\=\\&makes\\[\\]\\=toyota\\&maximum_distance\\=all\\&models\\[\\]\\=toyota-4runner\\&page\\=1\\&page_size\\=20\\&stock_type\\=used\\&zip\\=04103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ebe38484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 00:13:30 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 00:13:30 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.8 (default, Apr 13 2021, 12:59:45) - [Clang 10.0.0 ], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2022-05-03 00:13:30 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n"
     ]
    },
    {
     "ename": "ReactorAlreadyInstalledError",
     "evalue": "reactor already installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorAlreadyInstalledError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-27e1f290bfd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarilynMansonQuotes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(self, crawler_or_spidercls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;34m'The crawler_or_spidercls argument cannot be a spider object, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 'it must be a spider class (or a Crawler object)')\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mcrawler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawler_or_spidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mcreate_crawler\u001b[0;34m(self, crawler_or_spidercls)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawler_or_spidercls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcrawler_or_spidercls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawler_or_spidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36m_create_crawler\u001b[0;34m(self, spidercls)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspidercls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mspidercls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspider_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspidercls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_reactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_after_crawl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstall_signal_handlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spidercls, settings, init_reactor)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mlog_reactor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreactor_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/twisted/internet/selectreactor.py\u001b[0m in \u001b[0;36minstall\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minstallReactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0minstallReactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/twisted/internet/main.py\u001b[0m in \u001b[0;36minstallReactor\u001b[0;34m(reactor)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"twisted.internet.reactor\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyInstalledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reactor already installed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"twisted.internet.reactor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReactorAlreadyInstalledError\u001b[0m: reactor already installed"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class MarilynMansonQuotes(scrapy.Spider):\n",
    "    name = \"MarilynMansonQuotes\"\n",
    "    start_urls = [\n",
    "        'https://en.wikiquote.org/wiki/Marilyn_Manson',\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
    "            yield {'quote': quote.extract()}\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MarilynMansonQuotes)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85673d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 https://www.cars.com/shopping/results/?list_price_max=&makes[]=toyota&maximum_distance=all&models[]=toyota-4runner&page=1&page_size=20&stock_type=used&zip=04103>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
